{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3f0912",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/digital fellowships_dl.png\" width=\"100%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3156c1",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "<table style=\"float:right;\">\n",
    "    <tr>\n",
    "        <td>                      \n",
    "            <div style=\"text-align: right\"><a href=\"https://alandavies.netlify.com\" target=\"_blank\">Dr Alan Davies</a></div>\n",
    "            <div style=\"text-align: right\">Lecturer health data science</div>\n",
    "            <div style=\"text-align: right\">University of Manchester</div>\n",
    "         </td>\n",
    "         <td>\n",
    "             <img src=\"https://github.com/i3hsInnovation/resources/blob/efa61022d0b8893200dad308f6590e694291f8c7/images/alan.PNG?raw=true\" width=\"30%\" />\n",
    "         </td>\n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a27d0a",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks & Deep learning\n",
    "****\n",
    "\n",
    "### About this notebook\n",
    "This notebook gives an overview of the deep learning algorithm. It will focus on the concepts of deep learning and artificial neural networks and provide examples using the tensorflow package on the MNIST (Modified National Institute of Standards and Technology database) dataset.  It will explore Convolutional Neural Networks (CNNs) as well as giving you a taster of some basic coding in this area to help raise your awareness and bring this complicated topic to life.  This notebooks is at <code>Beginner</code> level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eed9f4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Learning Objectives:</b> \n",
    "    \n",
    "- Explain the concepts of artificial neural networks and introduce deep learning\n",
    "    \n",
    "- Investigate the basic modelling process for the creation of artificial neural networks\n",
    "\n",
    "- Explore some of the parameters that can be used to fine tune these networks\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7c385",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<b>Table of contents</b><br/>\n",
    "\n",
    "1.0 [What is Deep Learning?](#dl)\n",
    "\n",
    "2.0 [Activation Functions](#activation)\n",
    "\n",
    "3.0 [Deep Learning in Practice](#doing)\n",
    "\n",
    "4.0 [Convolutional Neural Networks](#conv)\n",
    "\n",
    "5.0 [Your Turn](#yourturn)\n",
    "\n",
    "6.0 [Advantages and Disadvantages](#proscons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4f335",
   "metadata": {},
   "source": [
    "<a id=\"dl\"></a>\n",
    "\n",
    "## What is Deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d00fdb",
   "metadata": {},
   "source": [
    "Deep Learning (DL) is a sub-set of machine learning methods based on the concept of artificial neural networks. These are networks that were designed to model how biological neurons work in living organisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcebb8e",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"./Topol/dl.png\" width=\"75%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095ee3d",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Biological neurons (nerve cells) receive and transmit electrochemical signals. Mammals (depending on species) have between 100 million and 100 billion neurons. \n",
    "\n",
    "A series of these interconnected neurons from neural pathways to carry signals form one part of the brain to another. These connections process the information that we receive. \n",
    "\n",
    "Neural pathways develop via frequent activation. This is analogous to walking down a familiar path many times until the route is so well known you can travel it blindfolded. These neural pathways can also be adapted (this ability is referred to as neuroplasticity). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bb387",
   "metadata": {},
   "source": [
    "(<strong>Source:</strong> Wikipedia.org CC BY-SA 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41473c9a",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "The image on the right shows a biological neuron with its key features labeled. This biological principle inspired the creation of <code>artificial neural networks</code> (ANNs) which attempt to mimic this process artificially. They emerged, in their simplest form, in the 1950s but were developed significantly from advances in cognitive and computer science in the 1980s. More recently computational performance and affordable hardware have made it possible to create more complex networks that would not have been feasible in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0539fe5",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"./Topol/neuron.PNG\" width=\"100%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a51fbd",
   "metadata": {},
   "source": [
    "<a id=\"perceptron\"></a>\n",
    "\n",
    "### 1.1 The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963dc79",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "One of the simplest ANN architectures is that of the <code>Perceptron</code> (image on right). This is an algorithm for supervised learning of binary classifiers (2 classes). This was actually created back in 1958 by Frank Rosenblatt an American psychologist who worked in the field of artificial intelligence. Here neurons in the network are referred to as <code>threshold logic units</code> or TLUs. Each input has an associated weight which the TLU computes the combined sum of from all the inputs. A <code>step function</code> is then applied (typically the Heaviside step function). If the output of the step function exceeds the threshold value then it outputs the positive class, otherwise the negative class. Training of Perceptrons was based on the concept of <code>Hebbian</code> learning where the weight of connecting neurons is increased if they have the same output. It turns out that there were some limitations to the Perceptron, meaning they were incapable of solving some relatively trivial problems. This was overcome however by stacking multiple Perceptron's to create a <code>Multi-Layer Perceptron</code> (MLP). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946addc8",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"./Topol/per.PNG\" width=\"100%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1b7f6",
   "metadata": {},
   "source": [
    "<a id=\"ann\"></a>\n",
    "### 1.2 The Multi-layered Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269e44f",
   "metadata": {},
   "source": [
    "Artificial neural networks are constructed in layers. The <code>input layer</code> ($x_1$ to $x_3$ in the image below). They then have a hidden layer (in a shallow network) and an output layer. Deep learning is essentially a ANN with more than one hidden layer. The circles in the image represent the individual neurons in the network. In this case one used to classify cancer or not based on 3 inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecdd853",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/nn.gif\" width=\"60%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c895aa40",
   "metadata": {},
   "source": [
    "These neurons hold a value between 0 and 1 (called its activation). The input layer could be many things, for example there could be an input representing every pixel in an image file. The output layer represents the different categories of the thing you are trying to predict (in this example, cancer or no-cancer). You can see how many connections are formed between the neurons of this very basic example with only 8 neurons. The numbers (activation) in the output layer represent how likely the system thinks the input corresponds with the output. In this example there are 2 hidden layers with 4 neurons in each layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfa652",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/nn2.PNG\" width=\"60%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccdf38",
   "metadata": {},
   "source": [
    "Activations that occur in one layer impact on the activations in subsequent layers. This works because each of the lines has an associated <code>weight</code> (a numerical value). The weighted sum of values has to exceed some threshold in order to activate the neuron. To make this summed value somewhere in the range of 0 to 1 we use a <code>sigmoid function</code> ($\\sigma$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426e9c9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> We can sum the weights, adding a bias term $b$ if required. This is how high the weighted sum needs to be before it gets meaningfully active. The Sigmoid function can be used as a squashing function: \n",
    "$$\\sigma(w_1 a_1 + w_2 a_2 + w_3 a_3, + \\dots + w_n a_n, b) $$\n",
    "The Sigmoid function: $$\\sigma(x) = \\frac{1}{1+\\epsilon^{-x}} $$\n",
    "means that the more negative the input is, the closer it is to 0. The more positive the input, the closer it is to 1. This is also referred to as a logistic curve. We can also add neurons that are deliberately biased (e.g. always output 1) to a network which are  used to adjust the output along with the weighted sum of the inputs.</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd13d1b",
   "metadata": {},
   "source": [
    "When these activation threshold is reached, the neuron 'fires'. The strength of the firing (the value generated by the neuron) determines its impact on subsequent neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841b04c",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/nn3.gif\" width=\"60%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132d92d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> Certain types of network work better for different kinds of task. For example <code>convolutional neural networks</code> work well for image recognition tasks whereas <code>Long short-term memory networks</code> are good for tasks like speech recognition.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6b811",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"#top\">Back to top</a>\n",
    "\n",
    "----------------------\n",
    "\n",
    "<a id=\"activation\"></a>\n",
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf3dc2",
   "metadata": {},
   "source": [
    "To better visualise the commonly used activation functions, we can plot them using Python. The following code cell creates these functions and then plots them. The resulting plots show what these functions look like when output as plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c917020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def binary_step(x):\n",
    "    return np.heaviside(x, 1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def RELU(x):\n",
    "    x1=[]\n",
    "    for i in x:\n",
    "        if i < 0:\n",
    "            x1.append(0)\n",
    "        else:\n",
    "            x1.append(i)\n",
    "    return x1\n",
    "\n",
    "x = np.linspace(-10, 10)\n",
    "fig, plts = plt.subplots(2, 3)\n",
    "fig.suptitle('Common activaton functions')\n",
    "plts[0, 0].plot(x, linear(x), color='red')\n",
    "plts[0, 0].set_title(\"Linear\")\n",
    "plts[0, 1].plot(x, binary_step(x), color='red')\n",
    "plts[0, 1].set_title(\"binaryStep\")\n",
    "plts[0, 2].plot(x, sigmoid(x), color='red')\n",
    "plts[0, 2].set_title(\"Sigmoid\")\n",
    "plts[1, 0].plot(x, tanh(x), color='red')\n",
    "plts[1, 0].set_title(\"TanH\")\n",
    "plts[1, 1].plot(x, RELU(x), color='red')\n",
    "plts[1, 1].set_title(\"ReLU\")\n",
    "plts[1, 2].plot(x, softmax(x), color='red')\n",
    "plts[1, 2].set_title(\"Softmax\")\n",
    "    \n",
    "for ax in plts.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade356e3",
   "metadata": {},
   "source": [
    "The most commonly used currently tends to be <code>ReLU</code> which stands for Rectified Linear Unit. The activation function that you choose can bias the network to certain types of data so it fits the data more appropriately therefore adjusting this accordingly can help improve the networks performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7630443",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> If you would like to know more about how the different activation functions work, take a look at this short blog post on <a href=\"https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\" target=\"_blank\">Activation Functions in Neural Networks</a>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765e52d",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to top</a>\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"doing\"></a>\n",
    "## Deep Learning in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840297bf",
   "metadata": {},
   "source": [
    "Before we look at implementing a Deep Learning model using Python, let's take a look at an online playground. This allow you to change some of the settings and parameters and visually see the effect on the network. Click on the link below to launch the playground and then complete the following tasks below (Task 1) using the playground. We will explore the settings in more detail later, for now this is to provide some intuition as to what the networks looks like and how they perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89506a",
   "metadata": {},
   "source": [
    "<a href=\"https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.36558&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\" target=\"_blank\">Deep Learning playground</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b81e588",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/play.PNG\" width=\"70%\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff3fb8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 1:</b>\n",
    "<br> \n",
    "Using the playground (link above), scroll down and read the section called \"What Do All the Colors Mean?\", then scroll back to the top and try the following. Note the effect these have on the network.<br>\n",
    "1. To use the network as it is click the play button at the top<br>\n",
    "2. Hover the mouse over the lines (to see the weights) and the boxes (the neurons)<br>\n",
    "3. Add another hidden layer and a few more neurons so there are 4 neurons in each hidden layer<br>\n",
    "4. Try changing the activation function<br>\n",
    "5. Try changing the dataset thats used and the ratio of training to test data<br>\n",
    "6. Finally change the problem type from classification to regression\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f86be",
   "metadata": {},
   "source": [
    "Now we have had a look at network we will use Python to build our own network. We will also take a deeper look at some of the settings you will have adjusted in the playground in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1deef",
   "metadata": {},
   "source": [
    "To create an ANN we will use the <code>tensorflow</code> package. There are several other popular Python modules we could have also used. TensorFlow is free open-source platform of tools for machine learning with a particular focus on deep neural networks. We will also use the <code>Keras</code> Python interface to TensorFlow. We can import it as follows, referring to it as <code>tf</code> (for TensorFlow). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b23eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df384aea",
   "metadata": {},
   "source": [
    "\n",
    "### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0715e",
   "metadata": {},
   "source": [
    "For the dataset we will use the <code>MNIST</code> data (Modified National Institute of Standards and Technology database). This dataset is often used when introducing deep learning. This consists of images of handwritten numbers 0 - 9 normalised by size to fit inside a 28x28 pixel box. They consist of a training set of 60,000 examples and a test set of 10,000 samples. This dataset is provided with tensorflow module and can be loaded into a variable as demonstrated below. If you want to read more about the dataset, you can find out more <a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ee87d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 1:</b>\n",
    "<br> \n",
    "    Go to the web-page <a href=\"https://knowyourdata-tfds.withgoogle.com/#tab=STATS&dataset=mnist\" target=\"_blank\">Know Your Data</a> and familiarise yourself with the features of the data.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495f5b4",
   "metadata": {},
   "source": [
    "Of course this data has been pre-prepared with labeled numbers showing the correct values for training and a test set for testing the accuracy of any model developed with the data. A lot of time goes into the early stages of collecting, cleaning and  labeling data before we can get to the model building stages. Many decisions here will also impact on the subsequent performance of your models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94535c",
   "metadata": {},
   "source": [
    "We can load the dataset from the TensorFlow datasets collection and store it in a variable called <code>mnist</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee8d35",
   "metadata": {},
   "source": [
    "Once loaded, we can split the data into training and test sets. Splitting by features <code>x</code> and labels <code>y</code> in the standard way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e740aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b28b1",
   "metadata": {},
   "source": [
    "Let's take a look at some of this data. We can plot 6 items in the dataset randomly to get an idea of what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ae6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, nums = plt.subplots(2, 3)\n",
    "fg.suptitle(\"Six random numbers from the MNIST dataset\")\n",
    "nums[0, 0].imshow(x_train[4], cmap = plt.cm.binary)\n",
    "nums[0, 1].imshow(x_train[0], cmap = plt.cm.binary)\n",
    "nums[0, 2].imshow(x_train[12], cmap = plt.cm.binary)\n",
    "nums[1, 0].imshow(x_train[105], cmap = plt.cm.binary)\n",
    "nums[1, 1].imshow(x_train[66], cmap = plt.cm.binary)\n",
    "nums[1, 2].imshow(x_train[75], cmap = plt.cm.binary)\n",
    "\n",
    "for ax in nums.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa70f549",
   "metadata": {},
   "source": [
    "If we wanted to see what this looks like as raw data, we can print one of the numbers (for example the first number we randomly chose which was a number 9). You can see that the pixel intensity values are contained within a relatively sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c14ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d9b57",
   "metadata": {},
   "source": [
    "If we take a look at the dimensions (<code>shape</code>) of the matrix, we can see it is a 28 by 28 matrix with the numbers representing the intensity of each of the pixels within the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[4].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76febb97",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> A pixel is the smallest addressable element in a raster image (a two-dimensional image represented as a rectangular matrix or grid of square pixels).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586dabb3",
   "metadata": {},
   "source": [
    "We can also check the dimensions of the training data features and labels. Here you can see there are 60000 28x28 images in the training set and 10000 in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59106ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6460d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dcf1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678ca9d",
   "metadata": {},
   "source": [
    "An optional step that helps to make it easier for the network to learn is to normalise the input. This is especially useful if the input can be of different magnitudes (e.g. house price or age). Here we scale the values using an inbuilt function so they are between 0 and 1. We apply this scaling to both the training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838622f",
   "metadata": {},
   "source": [
    "\n",
    "### Model Creation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2943b5",
   "metadata": {},
   "source": [
    "Next we can create the model itself. We will call this <code>dl_model</code> for deep learning model. The <code>Sequential</code> function creates a simple feed forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db32cd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> There are other kinds of architecture like recurrent networks that feed the connections in cycles. This shouldn't be confused with terms like Backpropagation that are used for training and involve propagating back the error to previous layers.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a763b12",
   "metadata": {},
   "source": [
    "Next we can use the <code>add</code> function to add new layers to the network. Here we define that the first layer will be a flat layer to flatten the input. This takes multi-dimensional input tensors (e.g. our 28 by 28 matrix/grid of pixels) and flattens it into a single dimension so the input can be passed into every neuron in the model effectively. So the input layer will consist of 784 values (28x28) representing each pixel of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e804aca",
   "metadata": {},
   "source": [
    "We can then add 2 more hidden layers and an output layer using the <code>add</code> function. Here we specify that we want dense layers (take 1D vectors as input) and that both the hidden layers will contain 16 neurons. We will also use the Rectified Linear Unit (ReLU) as the activation function for the hidden layers. Finally we have the output layer which will have 10 neurons to represent the numbers 0 to 9. Here the activation functions will be <code>softmax</code> because we want a probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model.add(tf.keras.layers.Dense(16, activation=tf.nn.relu)) \n",
    "dl_model.add(tf.keras.layers.Dense(16, activation=tf.nn.relu)) \n",
    "dl_model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1014d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong> How to choose how many neurons? The number of neurons in the input layer should equal the number of input variables being processed. The output layer should have the same number of output associated with the input (e.g. if wanting to predict cat or dog it would be 2, if predicting numbers 0 to 9, it would be 10 etc.). The hidden layers are more problematic and tend to follow rules of thumb. Typically one hidden layer is sufficient for the large majority of problems. The number of neurons in these layers is typically somewhere between the size of the input and output layers. Once built, you can fine-tune these details to improve the performance of the network.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978773bf",
   "metadata": {},
   "source": [
    "The model can now be compiled. Here we use the <code>sparse_categorical_crossentropy</code>. The <code>categorical_crossentropy</code> is one of the most commonly used loss options. We use the sparse version as we have sparse data. The optimizer is a method for minimizing the loss function (also called an error function). The <code>Adam</code> optimizer (Adaptive Moment Estimation) is also one of the most commonly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f3416",
   "metadata": {},
   "source": [
    "Loss (also known as cost) takes in to account all the weights, biases and outputs a single number. This works by findings an input that minimises the value of the lost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104320b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> The Adam optimizer extends stochastic gradient descent. For more information about optimizers, take a look at this blog post <a href=\"https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/\" target=\"_blank\">A Comprehensive Guide on Deep Learning Optimizers</a>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13cae4f",
   "metadata": {},
   "source": [
    "Now we can train the model. We do this using the <code>fit</code> function passing in the training data (the features and labels). We can then set the number of epochs (how many times the dataset passed through the network). The number of epochs tends to be related to the diversity of the data. The more diverse it is, the more epochs are required. We will store this in a variable called <code>history</code> for plotting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dl_model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89296c39",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 2:</b>\n",
    "<br> \n",
    "    Change the number of neurons in the first 2 layers in the code above by increasing the number (e.g. try 128 neurons) and run the code again. What effect does this have on the accuracy? Do you need 10 epochs or can you reduce the number?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3901c",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "You can increase the number of neurons, the network then requires less epochs. There is a degree of fine-tuning required to come up with the optimal network for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9e012",
   "metadata": {},
   "source": [
    "\n",
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be762a",
   "metadata": {},
   "source": [
    "We can use the <code>summary</code> function to see details about the network we have created including the number of parameters it can use for training. Even this relatively simple network has over 13 thousand parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a816f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1f3a6",
   "metadata": {},
   "source": [
    "We can also output the validation loss and accuracy using the <code>evaluate</code> function. We want to minimize the value of the loss function and increase the accuracy of the model. Here we can see that we have around 95% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss, validation_accuracy = dl_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ffb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768125e5",
   "metadata": {},
   "source": [
    "Finally, we can plot these values as another way of visualising the models performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d526b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20180556",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> Another way to get some sense of how the model is performing is to visualize a heatmap to see which parts of the image the model is paying most attention to. If you want to read more about how to do this with TensorFlow, check out this <a href=\"https://medium.com/analytics-vidhya/visualizing-activation-heatmaps-using-tensorflow-5bdba018f759\" target=\"_blank\">Visualizing Activation Heatmaps using TensorFlow</a> blog post.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8fa2d",
   "metadata": {},
   "source": [
    "There are many available options to choose from in terms of loss functions, optimizers and metrics to report. This depends on your data, performance and what you are trying to report. The table below highlights some of the available choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b995b05",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-kiyi{font-weight:bold;border-color:inherit;text-align:left}\n",
    ".tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-xldj{border-color:inherit;text-align:left}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-kiyi\">Loss functions</th>\n",
    "    <th class=\"tg-kiyi\">Optimizers</th>\n",
    "    <th class=\"tg-kiyi\">Metrics</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">mean_squared_error</td>   \n",
    "    <td class=\"tg-0pky\">Stochastic gradient descent (SGD)</td>\n",
    "    <td class=\"tg-0pky\">accuracy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">mean_absolute_error</td>   \n",
    "    <td class=\"tg-0pky\">RMSProp</td>\n",
    "    <td class=\"tg-0pky\">binary_accuracy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">mean_absolute_percentage_error</td>   \n",
    "    <td class=\"tg-0pky\">Adagrad</td>\n",
    "    <td class=\"tg-0pky\">categorical_accuracy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">mean_squared_logarithmic_error</td>   \n",
    "    <td class=\"tg-0pky\">Adadelta</td>\n",
    "    <td class=\"tg-0pky\">sparse_categorical_accuracy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">squared_hinge</td>   \n",
    "    <td class=\"tg-0pky\">Adam</td>\n",
    "    <td class=\"tg-0pky\">top_k_categorical_accuracy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">hinge</td>   \n",
    "    <td class=\"tg-0pky\">Adamax</td>\n",
    "    <td class=\"tg-0pky\">sparse_top_k_categorical_accuracy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">categorical_hinge</td>   \n",
    "    <td class=\"tg-0pky\">Nadam</td>\n",
    "    <td class=\"tg-0pky\">cosine_proximity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">logcosh</td>   \n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">clone_metric</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">huber_loss</td>   \n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">categorical_crossentropy</td>   \n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">sparse_categorical_crossentropy</td>   \n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">binary_crossentropy</td>   \n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">kullback_leibler_divergence</td>   \n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td class=\"tg-xldj\">poisson</td>   \n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td class=\"tg-xldj\">cosine_proximity</td>   \n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td class=\"tg-xldj\">is_categorical_crossentropy</td>   \n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd47e0",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"#top\">Back to top</a>\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "<a id=\"conv\"></a>\n",
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b288ece",
   "metadata": {},
   "source": [
    "In the example above we essentially used a very basic feed forward Multi-Level Perceptron to classify the images. This worked pretty well but the information in the images wasn't especially sophisticated. We took the pixel data in the form of 2D matrix and flattened it into a single vector of values to use an input layer. Convolutional Neural Networks (CNNs) are a better choice for image classification tasks. CNNs are capable of retaining the spacial and temporal dependencies in image data (e.g. the colour channels, width and height information, ...). CNNs can reduce images into a format that is faster and more efficient to process whilst retaining the important information. It does this by producing <code>convolved features</code> which are used to extract high level features like the edges of an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10829d0c",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/cnn.PNG\" width=\"50%\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb33ba3",
   "metadata": {},
   "source": [
    "Where images are concerned, pixels that are closer together are more strongly correlated than those that are more distant. We can extract local features (smaller sub-regions of an image) which can be merged later to detect higher order features. We can add additional convolutional layers to a network. Another way to think about this is as a series of filters being applied to an image. The filter (kernel) moves across the image and creates a <code>feature map</code> as shown in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3dd436",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/conv.gif\" width=\"60%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851ee63",
   "metadata": {},
   "source": [
    "The topic of CNNs is multi-faceted and beyond the introduction of deep learning presented in this notebook. If you want more information on CNNs, take a look at this short blog post <a href=\"https://towardsdatascience.com/a-beginners-guide-to-convolutional-neural-networks-cnns-14649dbddce8\" target=\"_blank\">A Beginner’s Guide to Convolutional Neural Networks (CNNs)</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db02b54",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> In practice, multiple filters are used in parallel (for example the multiple colour channels in an image such as red, green and blue). We can also use a technique called <code>pooling</code> to reduce the size of the inputs. For example we could take the max value in a 2x2 sub grid of a 4x4 matrix and store the output.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de073d62",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"#top\">Back to top</a>\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "<a id=\"yourturn\"></a>\n",
    "## Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d2f2c",
   "metadata": {},
   "source": [
    "For this, we will use a different dataset. This is the <code>CIFAR-10 dataset</code>. This consists of 60000 32x32 colour images that are split into 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck). There a 6000 images per class with 50000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548068b",
   "metadata": {},
   "source": [
    "We will get you started by loading in the dataset and normalising the pixel values to be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316ef9e",
   "metadata": {},
   "source": [
    "First we need to import a module to allow the notebook to connect and download the dataset to prevent an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test_images = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7a655",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 3:</b>\n",
    "<br>\n",
    "Using the cells above as an example enter code in the cells below to:<br>    \n",
    "    Output 6 of the images to see what they look like (<strong>Hint:</strong> you should remove the <code>cmap = plt.cm.binary</code> option as these are colour images).<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac734b60",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "fg1, nums1 = plt.subplots(2, 3)\n",
    "fg1.suptitle(\"Six random numbers from the cifar10 dataset\")\n",
    "nums1[0, 0].imshow(x_train[4])\n",
    "nums1[0, 1].imshow(x_train[6])\n",
    "nums1[0, 2].imshow(x_train[12])\n",
    "nums1[1, 0].imshow(x_train[105])\n",
    "nums1[1, 1].imshow(x_train[66])\n",
    "nums1[1, 2].imshow(x_train[75])\n",
    "\n",
    "for ax in nums.flat:\n",
    "    ax.label_outer()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490ad7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2616e33",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 4:</b>\n",
    "<br>\n",
    "1. Create a sequential model<br>\n",
    "2. Add a flattened input layer<br>\n",
    "3. Add at least 2 hidden layers<br>\n",
    "4. Add the output layer<br>\n",
    "5. Choose the number of neurons and an activation functions for the layers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a90707",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "dl_model1 = tf.keras.models.Sequential()\n",
    "dl_model1.add(tf.keras.layers.Flatten())\n",
    "dl_model1.add(tf.keras.layers.Dense(120, activation=tf.nn.relu)) \n",
    "dl_model1.add(tf.keras.layers.Dense(120, activation=tf.nn.relu)) \n",
    "dl_model1.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20503884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52226fed",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 5:</b>\n",
    "<br>\n",
    "1. Compile the model. Here you can change the optimizer and or the loss parameters if you want to<br>\n",
    "2. Train the model (how many epochs will you choose?)<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f50fe",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "dl_model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history1 = dl_model1.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d1c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d397d0e4",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 6:</b>\n",
    "<br>\n",
    "Evaluate the model and print out the validation loss and validation accuracy. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ab5fc",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "validation_loss, validation_accuracy = dl_model1.evaluate(x_test, y_test)\n",
    "print(validation_loss)\n",
    "print(validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d761a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce70b32a",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 7:</b>\n",
    "<br>\n",
    "The accuracy is not very high. Given the type of data we are using and its features, what we could we do to get a better result? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de42c3",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Given that we are using colour images that contain more complex information, we could try using a Convolutional Neural Network (CNN) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb595580",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0a54a7723517>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Compile and train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mCNN_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mCNN_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "#import layers\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# build the architecture \n",
    "CNN_model = models.Sequential()\n",
    "CNN_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "CNN_model.add(layers.MaxPooling2D((2, 2)))\n",
    "CNN_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "CNN_model.add(layers.MaxPooling2D((2, 2)))\n",
    "CNN_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "CNN_model.add(layers.Flatten())\n",
    "CNN_model.add(layers.Dense(64, activation='relu'))\n",
    "CNN_model.add(layers.Dense(10))\n",
    "\n",
    "# Compile and train the model\n",
    "CNN_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n",
    "CNN_model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = CNN_model.evaluate(x_test, y_test)\n",
    "print(\"Loss:\", val_loss)\n",
    "print(\"Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c40dd",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If you want more information about Convolutional Neural Networks (CNNs) and some of the functions used in the example above (e.g. <code>Conv2D</code> and <code>MaxPooling2D</code>, then have a look a this short blog post <a href=\"https://towardsdatascience.com/a-beginners-guide-to-convolutional-neural-networks-cnns-14649dbddce8\" target=\"_blank\">A Beginner’s Guide to Convolutional Neural Networks (CNNs)</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf431be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33555499",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to top</a>\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "<a id=\"proscons\"></a>\n",
    "## Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497c09a",
   "metadata": {},
   "source": [
    "The main advantages of artificial neural networks include:\n",
    "<ul>\n",
    "    <li>They can be applied to both classification and numerical prediction problems</li>\n",
    "    <li>They make minimal assumptions about the underlying data and relationships</li>\n",
    "    <li>They can be used to model complex problems</li>\n",
    "</ul>\n",
    "<br>\n",
    "There are of course some disadvantages to their use as well. These include:\n",
    "<ul>\n",
    "    <li>They are prone to over-fitting training data</li>\n",
    "    <li>They are slow to train and computationally intensive which often requires more resource. This is increased with the complexity of the network</li>\n",
    "    <li>It's not always clear how the results are obtained (the black box effect) which makes explainability difficult. This is especially problematic in medical/health settings where evidence and rationale are vital for decision making and accountability</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750bdc7",
   "metadata": {},
   "source": [
    "This has been a fairly basic introduction to the concepts of artificial neural networks and deep learning. As with many of the machine learning algorithms introduced there is a lot of nuance in the decisions that you make. For example the topology of the network, its number of layers, the type (Multi-Layer Perceptrons, Convolutional/Recurrent Neural Networks), the activation functions and optimizers. ANNs can also be saved and used as the inputs or layers of other networks leading to complex topologies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5432cb",
   "metadata": {},
   "source": [
    "#### Further Reading\n",
    "\n",
    "A collection of cheat sheets on various ML and data science topics: <a href=\"https://www.bigdataheaven.com/wp-content/uploads/2019/02/AI-Neural-Networks.-22.pdf\" target=\"_blank\">AI and neural networks</a>\n",
    "\n",
    "Géron, A. (2019) <u><i>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Ed</i></u> O'Reilly, ISBN: 9781492032649</li> <a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\" target=\"_blank\">Link</a>\n",
    "\n",
    "------\n",
    "\n",
    "##### About this Notebook\n",
    "<br>\n",
    "<i>Notebook created by <strong>Dr. Alan Davies</strong> with, <strong>Frances Hooley</strong> and <strong>Dr. Jon Parkinson</strong>\n",
    "\n",
    "Publish date: November 2021<br>\n",
    "Review date: November 2022</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ba86a",
   "metadata": {},
   "source": [
    "<a class=\"typeform-share button\" href=\"https://hub11.typeform.com/to/kxuGb4Jv\" data-mode=\"popup\" style=\"display:inline-block;text-decoration:none;background-color:#3A7685;color:white;cursor:pointer;font-family:Helvetica,Arial,sans-serif;font-size:18px;line-height:45px;text-align:center;margin:0;height:45px;padding:0px 30px;border-radius:22px;max-width:100%;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;font-weight:bold;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;\" target=\"_blank\">Rate this notebook </a> <script> (function() { var qs,js,q,s,d=document, gi=d.getElementById, ce=d.createElement, gt=d.getElementsByTagName, id=\"typef_orm_share\", b=\"https://embed.typeform.com/\"; if(!gi.call(d,id)){ js=ce.call(d,\"script\"); js.id=id; js.src=b+\"embed.js\"; q=gt.call(d,\"script\")[0]; q.parentNode.insertBefore(js,q) } })() </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76c239",
   "metadata": {},
   "source": [
    "## Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87282a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f772588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
